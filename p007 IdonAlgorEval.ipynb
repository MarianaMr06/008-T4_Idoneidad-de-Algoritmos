{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jhM_z0o04VMjf4Xyg39MPu0j5pKTI5M5","timestamp":1697432372955}],"authorship_tag":"ABX9TyOA+UdXJj7u5qnAToMCBLG4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Evaluación, Simulación y Validación de Idoneidad\n","\n","Este tema es interesante por que podemos hablar de cosas como: partición de la muestra, programación de las soluciones, sobre y supra parametrización (overfitting y underfitting), validación cruzada, bootstrapping, etc."],"metadata":{"id":"2Vafdd6yPhi8"}},{"cell_type":"markdown","source":["##Partición de Muestras y validación cruzada.\n","Estos dos temas son importantes, primer debemos entender ¿Porqué realizamos esto? (Tomen notas)\n"],"metadata":{"id":"koEpMsKRQ67Q"}},{"cell_type":"markdown","source":["###Partición de muestras\n","La idea es simple, ¿Cómo dividir un dataset entre el conjunto de prueba y el conjunto de entrenamiento? Esto es super sencillo en Python."],"metadata":{"id":"nBNv2ZwyFLaT"}},{"cell_type":"code","source":["#Hagamos algunas cosas simples con Scikit-Learn\n","#Empecemos con sets de prueba y entrenamiento.\n","#Este es el código base para particionar en test y train sets.\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","#Tenemos que insertar un conjunto de datos, separaremos el 20% como el test set y el random_state es\n","#Para tener reproducibilidad."],"metadata":{"id":"M1zTPOA2RRhN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A continuación un breve ejemplo para que vean cómo se utiliza esta función:"],"metadata":{"id":"rIvma767Fcgv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJi99gZwOb9a"},"outputs":[],"source":["#Veamos un ejemplo:\n","#Librerias:\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","# Generamos datos:\n","np.random.seed(0)\n","X = np.random.rand(100, 2)\n","# Clasificación simple (1 si x1 + x2 > 1, 0 en caso contrario)\n","y = (X[:, 0] + X[:, 1] > 1).astype(int)\n","\n","# Definimos el número de particiones\n","particiones = 5\n","\n","# Creamos subplots para visualizar las particiones\n","fig, axes = plt.subplots(1, particiones, figsize=(15, 3))\n","\n","# Realizamos múltiples particiones y visualizamos\n","for i in range(particiones):\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i * 42)  # Diferente semilla para cada partición\n","\n","    # Visualizamos los puntos de entrenamiento y prueba en el scatterplot\n","    ax = axes[i]\n","    ax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='Clase 1', marker='o')\n","    ax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='Clase 0', marker='x')\n","    ax.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], label='Prueba - Clase 1', marker='s')\n","    ax.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], label='Prueba - Clase 0', marker='d')\n","\n","    ax.set_title(f'Partición {i+1}')\n","    ax.set_xlabel('Característica 1')\n","    ax.set_ylabel('Característica 2')\n","    if i == 0:\n","        ax.legend(loc='upper right')\n","    else:\n","        ax.legend().set_visible(False)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","source":["###Validación Cruzada\n","La validación cruzada es un método de análisis que sirve para evaluar un modelo, utiliza el mismo proceso de partición de la DB en entrenamiento y prueba y simplemente lo repite K veces. ¿Para qué sirve utilizar este método?"],"metadata":{"id":"AENlonFPFnR-"}},{"cell_type":"code","source":["## Este sería el código base para utilizar la validación cruzada\n","#(sigue siendo parte de la librería scikit.learn)\n","from sklearn.model_selection import cross_val_score\n","#Esta función sirve para realizar la validación cruzada.\n","scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n","print(\"Scores:\", scores)\n","print(\"Promedio:\", scores.mean())"],"metadata":{"id":"Du1dTvCdGO5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora veamos un ejemplo visual al respecto de la validación cruzada."],"metadata":{"id":"5njU6_84GoKD"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Generamos datos de ejemplo\n","X, y = make_classification(n_samples=100, n_features=2,n_informative=2,n_redundant=0, random_state=345)\n","\n","# Definimos un clasificador (puedes usar cualquier modelo)\n","model = RandomForestClassifier(n_estimators=10, random_state=2342)\n","\n","# Realizamos validación cruzada con 5 pliegues\n","k = 5\n","scores = cross_val_score(model, X, y, cv=k, scoring='accuracy')\n","####Podemos analizar con CV cosas como\n","# 'f1' para el puntaje F1\n","# 'recall' para la sensibilidad\n","# 'precision' para la precisión.\n","# 'roc_auc' para calcular el área bajo la curva de la curva ROC.\n","\n","# Gráfico de barras para visualizar los resultados\n","plt.bar(range(1, k + 1), scores)\n","plt.xlabel('Pliegues')\n","plt.ylabel('Exactitud (Accuracy)')\n","plt.title('Resultados de Validación Cruzada (Accuracy)')\n","plt.show()\n"],"metadata":{"id":"qH0VrxcoGrfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Overfitting (Sobreajuste) y Underfitting (Subajuste)\n","Los pecados mortales de la modelación.\n","\n","Underfitting: El modelo es demasiado sencillo para representar adecuadamente los datos y no puede hacer predicciones precisas.\n","\n","Overfitting: El modelo es demasiado complejo y se adapta demasiado a los datos de entrenamiento, perdiendo su capacidad de generalización."],"metadata":{"id":"QNktvrsIJHvL"}},{"cell_type":"code","source":["#Underfitting: Subajuste\n","#Utilizando de forma incorrecta un modelo lineal simple.\n","#Librerías\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","# Generamos datos de ejemplo\n","np.random.seed(0)\n","X = np.sort(5 * np.random.rand(80, 1), axis=0)\n","y = np.sin(X).ravel() + np.random.rand(80)\n","\n","#Partición de sets de datos:\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=311)\n","\n","# Ajustamos un modelo de regresión lineal\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","#Revisión y cálculo del MSE (Error cuadrado medio)\n","y_pred = model.predict(X_test)\n","mse_underfit = mean_squared_error(y_test, y_pred)\n","print(f\"MSE - Underfit: {mse_underfit}\")\n","\n","# Visualizamos los datos y la predicción del modelo\n","plt.scatter(X, y, color='darkorange', label='Datos')\n","plt.plot(X, model.predict(X), color='cornflowerblue', label='Modelo')\n","plt.xlabel('Característica')\n","plt.ylabel('Etiqueta')\n","plt.title('Underfitting: Regresión Lineal Simple')\n","plt.legend(loc='best')\n","plt.show()\n"],"metadata":{"id":"yBj0GnkDZhp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Overfitting: Un modelo hiperparametrizado\n","#árbol de decisión con alta complejidad.\n","#Librerías\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Generamos datos de ejemplo\n","np.random.seed(0)\n","X = np.sort(5 * np.random.rand(80, 1), axis=0)\n","y = np.sin(X).ravel() + np.random.rand(80)\n","\n","#Partición de sets de datos:\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=311)\n","\n","# Ajustamos un modelo de árbol de decisión altamente complejo\n","model = DecisionTreeRegressor(max_depth=20)\n","model.fit(X_train, y_train)\n","\n","#Cálculo del MSE.\n","y_pred = model.predict(X_test)\n","mse_overfit = mean_squared_error(y_test, y_pred)\n","print(f\"MSE - Overfit: {mse_overfit}\")\n","\n","# Visualizamos los datos y la predicción del modelo\n","X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n","y_pred = model.predict(X_test)\n","plt.scatter(X, y, color='darkorange', label='Datos')\n","plt.plot(X_test, y_pred, color='cornflowerblue', label='Modelo')\n","plt.xlabel('Característica')\n","plt.ylabel('Etiqueta')\n","plt.title('Overfitting: Árbol de Decisión Complejo')\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"otfmoeFhZoeJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Métricas de evaluación\n","Hasta ahora todo ha sido muy bueno con el manejo de ciertos números que nos sirven para definir si un modelo hizo o no la \"chamba\". Recordaremos puntajes como exactitud, precisión, sensibilidad y puntaje F1."],"metadata":{"id":"JXR6hno0lGMZ"}},{"cell_type":"code","source":["#Los diversos puntajes que podemos obtener.\n","#Aquí no hay mucho que decir, ya los han calculado (clase pasada).\n","#Recuerden que este código les sirve para ver qué es lo que hay y cómo se implementa:\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","y_pred = model.predict(X_test)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","cm = confusion_matrix(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1 Score: {f1}\")\n","print(\"Confusion Matrix:\")\n","print(cm)\n"],"metadata":{"id":"DlXCyxCDlhoN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Este es el ejemplo:\n","#Librerías\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Generamos datos de ejemplo\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=433)\n","print(X.shape)\n","print(y.shape)\n","# Dividimos los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=82)\n","\n","# Entrenamos dos modelos: Regresión Logística y Random Forest\n","model_logistic = LogisticRegression()\n","model_rf = RandomForestClassifier()\n","\n","model_logistic.fit(X_train, y_train)\n","model_rf.fit(X_train, y_train)\n","\n","# Realizamos predicciones en el conjunto de prueba\n","y_pred_logistic = model_logistic.predict(X_test)\n","y_pred_rf = model_rf.predict(X_test)\n","\n","# Calculamos las métricas para el modelo de Regresión Logística\n","acc_logistic = accuracy_score(y_test, y_pred_logistic)\n","precision_logistic = precision_score(y_test, y_pred_logistic)\n","recall_logistic = recall_score(y_test, y_pred_logistic)\n","f1_logistic = f1_score(y_test, y_pred_logistic)\n","\n","print(\"Métricas para Regresión Logística:\")\n","print(\"Accuracy:\", acc_logistic)\n","print(\"Precision:\", precision_logistic)\n","print(\"Recall:\", recall_logistic)\n","print(\"F1 Score:\", f1_logistic)\n","\n","\n","# Calculamos las métricas para el modelo Random Forest\n","acc_rf = accuracy_score(y_test, y_pred_rf)\n","precision_rf = precision_score(y_test, y_pred_rf)\n","recall_rf = recall_score(y_test, y_pred_rf)\n","f1_rf = f1_score(y_test, y_pred_rf)\n","\n","# Calculamos y mostramos las métricas para el modelo Random Forest\n","print(\"\\nMétricas para Random Forest:\")\n","print(\"Accuracy:\", acc_rf)\n","print(\"Precision:\", precision_rf)\n","print(\"Recall:\", recall_rf)\n","print(\"F1 Score:\", f1_rf)\n","\n","\n","# Creamos y mostramos las matrices de confusión con Seaborn\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","cm_logistic = confusion_matrix(y_test, y_pred_logistic)\n","sns.heatmap(cm_logistic, annot=True, fmt=\"d\", cmap=\"Blues\")\n","plt.title(\"Matriz de Confusión - Regresión Logística\")\n","\n","plt.subplot(1, 2, 2)\n","cm_rf = confusion_matrix(y_test, y_pred_rf)\n","sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Greens\")\n","plt.title(\"Matriz de Confusión - Random Forest\")\n","\n","plt.show()\n","\n","# Creamos gráficos de barras para comparar las métricas\n","models = [\"Regresión Logística\", \"Random Forest\"]\n","acc_scores = [acc_logistic, acc_rf]\n","precision_scores = [precision_logistic, precision_rf]\n","recall_scores = [recall_logistic, recall_rf]\n","f1_scores = [f1_logistic, f1_rf]\n","\n","plt.figure(figsize=(10, 6))\n","x = np.arange(len(models))\n","bar_width = 0.2\n","\n","plt.bar(x, acc_scores, width=bar_width, label=\"Accuracy\", align=\"center\", alpha=0.7)\n","plt.bar(x + bar_width, precision_scores, width=bar_width, label=\"Precision\", align=\"center\", alpha=0.7)\n","plt.bar(x + 2 * bar_width, recall_scores, width=bar_width, label=\"Recall\", align=\"center\", alpha=0.7)\n","plt.bar(x + 3 * bar_width, f1_scores, width=bar_width, label=\"F1-Score\", align=\"center\", alpha=0.7)\n","\n","plt.xlabel(\"Modelos\")\n","plt.xticks(x + bar_width, models)\n","plt.title(\"Comparación de Métricas de Evaluación\")\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"CSYf0DZwnBV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Asterisco: Curva ROC-AUC\n","\n","La curva ROC (Receiver Operating Characteristic) y el área bajo la curva (AUC) se utilizan para evaluar el rendimiento del modelo. Es una manera de ver la capacidad del modelo de identificar correctamente la característica de interés."],"metadata":{"id":"TdODz5K9r9_9"}},{"cell_type":"code","source":["#Ejemplo de funciones a utilizar para la propuesta de curva ROC-AUC.\n","from sklearn.metrics import roc_curve, roc_auc_score\n","\n","y_scores = model.predict_proba(X_test)[:,1]\n","fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n","auc = roc_auc_score(y_test, y_scores)\n","\n","plt.plot(fpr, tpr, label=f'AUC = {auc}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"mtYCO2fPtZW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Ejemplo de ROC-AUC\n","import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","\n","# Generamos datos de ejemplo\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=672)\n","\n","# Dividimos los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=568)\n","\n","# Modelo con alto rendimiento (alto ROC-AUC)\n","model_high_auc = RandomForestClassifier(n_estimators=100, random_state=2664)\n","model_high_auc.fit(X_train, y_train)\n","y_pred_high_auc = model_high_auc.predict_proba(X_test)[:, 1]  # Probabilidad de clase positiva\n","\n","# Modelo con bajo rendimiento (bajo ROC-AUC)\n","model_low_auc = LogisticRegression()\n","model_low_auc.fit(X_train, y_train)\n","y_pred_low_auc = model_low_auc.predict_proba(X_test)[:, 1]  # Probabilidad de clase positiva\n","\n","# Calculamos las curvas ROC y el ROC-AUC para ambos modelos\n","fpr_high_auc, tpr_high_auc, _ = roc_curve(y_test, y_pred_high_auc)\n","roc_auc_high_auc = roc_auc_score(y_test, y_pred_high_auc)\n","\n","fpr_low_auc, tpr_low_auc, _ = roc_curve(y_test, y_pred_low_auc)\n","roc_auc_low_auc = roc_auc_score(y_test, y_pred_low_auc)\n","\n","# Visualizamos las curvas ROC\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(fpr_high_auc, tpr_high_auc, color='darkorange', lw=2, label='Modelo de Alto ROC-AUC (AUC = %0.2f)' % roc_auc_high_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Curva ROC - Random Forest')\n","plt.legend(loc='lower right')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(fpr_low_auc, tpr_low_auc, color='darkorange', lw=2, label='Modelo de Bajo ROC-AUC (AUC = %0.2f)' % roc_auc_low_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Curva ROC - Regresión Logística')\n","plt.legend(loc='lower right')\n","\n","plt.show()\n"],"metadata":{"id":"7H2Kd_e6t7Ed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluación en Regresión\n","Aunque hasta ahora hemos estado hablando de ciertas técnicas y algoritmos de clasificación, también tenemos que hablar de modelos de regresión para la predicción del comportamiento de una variable respuesta específica. A través de medidas como el MSE (Error Cuadrado Medio), el MAE (Error Cuadrado Absoluto) y el $R^2$ (coeficiente de determinación)."],"metadata":{"id":"JR9h6SGYvyui"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_diabetes\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.svm import SVR\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Cargamos el conjunto de datos \"Diabetes\"\n","diabetes = load_diabetes()\n","X = diabetes.data\n","y = diabetes.target\n","\n","# Dividimos los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Entrenamos tres modelos diferentes\n","model_linear = LinearRegression()\n","model_svr = SVR(kernel='linear')\n","model_tree = DecisionTreeRegressor(max_depth=5)\n","\n","model_linear.fit(X_train, y_train)\n","model_svr.fit(X_train, y_train)\n","model_tree.fit(X_train, y_train)\n","\n","# Realizamos predicciones en el conjunto de prueba\n","y_pred_linear = model_linear.predict(X_test)\n","y_pred_svr = model_svr.predict(X_test)\n","y_pred_tree = model_tree.predict(X_test)\n","\n","# Calculamos métricas de evaluación\n","mse_linear = mean_squared_error(y_test, y_pred_linear)\n","r2_linear = r2_score(y_test, y_pred_linear)\n","\n","mse_svr = mean_squared_error(y_test, y_pred_svr)\n","r2_svr = r2_score(y_test, y_pred_svr)\n","\n","mse_tree = mean_squared_error(y_test, y_pred_tree)\n","r2_tree = r2_score(y_test, y_pred_tree)\n","\n","# Visualizamos los resultados\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 3, 1)\n","plt.scatter(y_test, y_pred_linear)\n","#plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--k')  # Línea diagonal\n","plt.xlabel(\"Valores Reales\")\n","plt.ylabel(\"Predicciones\")\n","plt.title(f\"Regresión Lineal\\nMSE: {mse_linear:.2f}, R^2: {r2_linear:.2f}\")\n","\n","plt.subplot(1, 3, 2)\n","plt.scatter(y_test, y_pred_svr)\n","#plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--k')  # Línea diagonal\n","plt.xlabel(\"Valores Reales\")\n","plt.ylabel(\"Predicciones\")\n","plt.title(f\"Support Vector Machine (SVR)\\nMSE: {mse_svr:.2f}, R^2: {r2_svr:.2f}\")\n","\n","plt.subplot(1, 3, 3)\n","plt.scatter(y_test, y_pred_tree)\n","#plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--k')  # Línea diagonal\n","plt.xlabel(\"Valores Reales\")\n","plt.ylabel(\"Predicciones\")\n","plt.title(f\"Árbol de Decisión\\nMSE: {mse_tree:.2f}, R^2: {r2_tree:.2f}\")\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"4rEpcqQgwyOy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Consideraciones de Recursos Computacionales\n","\n","Espero que hayan leído el capítulo que se les dejó en el control de lectura.\n","\n","Las consideraciones de recursos son un aspecto crítico en la selección de algoritmos.\n"],"metadata":{"id":"AiCxB9E6ygPS"}},{"cell_type":"markdown","source":["Consideraciones que deben tener:\n","\n","**Recursos Computacionales:** La cantidad de recursos computacionales, como la potencia de cómputo y la memoria disponible, puede influir en la elección del algoritmo.\n","\n","**Tiempo de Entrenamiento:** Si tienes limitaciones de tiempo para obtener resultados, es importante seleccionar algoritmos que se ajusten a tu cronograma.\n","\n","**Escalabilidad:** La capacidad de un algoritmo para manejar conjuntos de datos de diferentes tamaños.\n","\n","**Requisitos de Memoria:** Algunos algoritmos pueden requerir grandes cantidades de memoria para cargar y procesar conjuntos de datos.\n","\n","**Condiciones de Producción:** Debes considerar si los resultados tienen una cierta necesidad de presentación con periodicidad específica.\n","\n","**Algoritmos Paralelizables:** Si tienes acceso a una infraestructura de cómputo paralelo o distribuido, puedes considerar algoritmos que se pueden paralelizar fácilmente para acelerar el entrenamiento.\n","\n","**Optimización de Hiperparámetros:** Algunos algoritmos pueden requerir una optimización extensa de hiperparámetros (Bayes, por ejemplo) para lograr un buen rendimiento.\n","\n","**Consideraciones de Almacenamiento:** Modelos complejos pueden ocupar más espacio de almacenamiento, y debes asegurarte de tener suficiente capacidad de almacenamiento disponible."],"metadata":{"id":"jJyQW2kz2K2_"}},{"cell_type":"code","source":["import time\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","import sys\n","\n","# Cargamos el conjunto de datos \"Breast Cancer\"\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Dividimos los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Consideración 1: Tiempo de Entrenamiento\n","# Comparamos el tiempo de entrenamiento de dos modelos: Bosques Aleatorios y SVM.\n","start_time = time.time()\n","\n","model_rf = RandomForestClassifier()\n","model_rf.fit(X_train, y_train)\n","\n","rf_training_time = time.time() - start_time\n","\n","start_time = time.time()\n","\n","model_svm = SVC(kernel='linear')\n","model_svm.fit(X_train, y_train)\n","\n","svm_training_time = time.time() - start_time\n","\n","print(f\"Tiempo de entrenamiento (Bosques Aleatorios): {rf_training_time:.2f} segundos\")\n","print(f\"Tiempo de entrenamiento (SVM): {svm_training_time:.2f} segundos\")\n","\n","# Consideración 2: Requisitos de Memoria\n","# Comparamos los requisitos de memoria para los dos modelos.\n","rf_memory_usage = sys.getsizeof(model_rf)\n","svm_memory_usage = sys.getsizeof(model_svm)\n","\n","print(f\"Requisitos de memoria (Bosques Aleatorios): {rf_memory_usage / (1024 * 1024):.2f} MB\")\n","print(f\"Requisitos de memoria (SVM): {svm_memory_usage / (1024 * 1024):.2f} MB\")\n","print(f\"Requisitos de memoria (Random Forests en bytes): {rf_memory_usage} bytes\")\n","print(f\"Requisitos de memoria (SVM en bytes): {svm_memory_usage} bytes\")"],"metadata":{"id":"bk7R4uD90vW8"},"execution_count":null,"outputs":[]}]}